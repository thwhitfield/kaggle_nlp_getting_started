{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with Disaster Tweets Kaggle\n",
    "1. Set up the experimentation framework\n",
    "    - Subset the train data into train/val/test\n",
    "    - Optimize the model on the train dataset, using val to evaluate performance per epoch\n",
    "    - Evaluate performance on test\n",
    "    - If performance is good enough submit that to kaggle and then see what the resulting output is\n",
    "    - I should be able to track the performance of a given set of hyperparameters or design decisions through the whole process. I.e. I'll get a train, val, test performance numbers, then I'll retrain it on the whole dataset using that approach, then I'll submit that to kaggle and evaluate the leaderboard performance for that submission and add it to the experiment tracker.\n",
    "2. Ok, that's all great. Let's see if I can structure my code in such a way that I can move the sort of repeatable, reuseable part of my code to one portion, and then have the custom code for reading in and preprocessing the data in a different place. That way I could theoretically swap out the preprocessing code and keep the model trainin code if I wanted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.create({\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train.csv'\n",
    "test_path = '../data/test.csv'\n",
    "sample_submission_path = '../data/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pl.read_csv(train_path)\n",
    "df_test = pl.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 5), (3263, 4))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>keyword</th><th>location</th><th>text</th><th>target</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>9565</td><td>&quot;thunder&quot;</td><td>&quot;Enfield, UK&quot;</td><td>&quot;#PlayingNow #BLOODBOUND Seven …</td><td>0</td></tr><tr><td>2743</td><td>&quot;crushed&quot;</td><td>null</td><td>&quot;So many Youtube commenters say…</td><td>1</td></tr><tr><td>1845</td><td>&quot;burned&quot;</td><td>&quot;956&quot;</td><td>&quot;It hurts for me to eat cause i…</td><td>0</td></tr><tr><td>8212</td><td>&quot;riot&quot;</td><td>null</td><td>&quot;@AcaciaPenn I&#x27;ll start a big a…</td><td>0</td></tr><tr><td>2730</td><td>&quot;crushed&quot;</td><td>&quot;Guayaquil&quot;</td><td>&quot;I crushed a 3.1 km run with a …</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────┬─────────┬─────────────┬─────────────────────────────────┬────────┐\n",
       "│ id   ┆ keyword ┆ location    ┆ text                            ┆ target │\n",
       "│ ---  ┆ ---     ┆ ---         ┆ ---                             ┆ ---    │\n",
       "│ i64  ┆ str     ┆ str         ┆ str                             ┆ i64    │\n",
       "╞══════╪═════════╪═════════════╪═════════════════════════════════╪════════╡\n",
       "│ 9565 ┆ thunder ┆ Enfield, UK ┆ #PlayingNow #BLOODBOUND Seven … ┆ 0      │\n",
       "│ 2743 ┆ crushed ┆ null        ┆ So many Youtube commenters say… ┆ 1      │\n",
       "│ 1845 ┆ burned  ┆ 956         ┆ It hurts for me to eat cause i… ┆ 0      │\n",
       "│ 8212 ┆ riot    ┆ null        ┆ @AcaciaPenn I'll start a big a… ┆ 0      │\n",
       "│ 2730 ┆ crushed ┆ Guayaquil   ┆ I crushed a 3.1 km run with a … ┆ 0      │\n",
       "└──────┴─────────┴─────────────┴─────────────────────────────────┴────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>keyword</th><th>location</th><th>text</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>7726</td><td>&quot;panicking&quot;</td><td>&quot;9.25.14?8.5.15?10.6.15 | gen?&quot;</td><td>&quot;this is from my show last nigh…</td></tr><tr><td>4839</td><td>&quot;evacuation&quot;</td><td>&quot;NIFC&quot;</td><td>&quot;#MadRiverComplex #CA #CASRF ht…</td></tr><tr><td>4746</td><td>&quot;evacuate&quot;</td><td>null</td><td>&quot;@yourgirlhaileyy leaveevacuate…</td></tr><tr><td>8076</td><td>&quot;rescue&quot;</td><td>null</td><td>&quot;@wcvh01 @1233newcastle @aaronk…</td></tr><tr><td>2247</td><td>&quot;chemical%20emergency&quot;</td><td>null</td><td>&quot;@bendwavy emergency chemical r…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────┬──────────────────────┬───────────────────────────────┬─────────────────────────────────┐\n",
       "│ id   ┆ keyword              ┆ location                      ┆ text                            │\n",
       "│ ---  ┆ ---                  ┆ ---                           ┆ ---                             │\n",
       "│ i64  ┆ str                  ┆ str                           ┆ str                             │\n",
       "╞══════╪══════════════════════╪═══════════════════════════════╪═════════════════════════════════╡\n",
       "│ 7726 ┆ panicking            ┆ 9.25.14?8.5.15?10.6.15 | gen? ┆ this is from my show last nigh… │\n",
       "│ 4839 ┆ evacuation           ┆ NIFC                          ┆ #MadRiverComplex #CA #CASRF ht… │\n",
       "│ 4746 ┆ evacuate             ┆ null                          ┆ @yourgirlhaileyy leaveevacuate… │\n",
       "│ 8076 ┆ rescue               ┆ null                          ┆ @wcvh01 @1233newcastle @aaronk… │\n",
       "│ 2247 ┆ chemical%20emergency ┆ null                          ┆ @bendwavy emergency chemical r… │\n",
       "└──────┴──────────────────────┴───────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>keyword</th><th>location</th><th>text</th><th>target</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>61</td><td>2533</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 5)\n",
       "┌─────┬─────────┬──────────┬──────┬────────┐\n",
       "│ id  ┆ keyword ┆ location ┆ text ┆ target │\n",
       "│ --- ┆ ---     ┆ ---      ┆ ---  ┆ ---    │\n",
       "│ u32 ┆ u32     ┆ u32      ┆ u32  ┆ u32    │\n",
       "╞═════╪═════════╪══════════╪══════╪════════╡\n",
       "│ 0   ┆ 61      ┆ 2533     ┆ 0    ┆ 0      │\n",
       "└─────┴─────────┴──────────┴──────┴────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.null_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer successfully saved to tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "def train_wordpiece_tokenizer_from_dataset(\n",
    "    csv_file_path,\n",
    "    vocab_size=30000,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    "    output_path=\"tokenizer.json\"\n",
    "):\n",
    "    # Load the CSV file as a dataset using Hugging Face Datasets library.\n",
    "    # This creates a default \"train\" split.\n",
    "    dataset = load_dataset(\"csv\", data_files=csv_file_path)\n",
    "    \n",
    "    # Ensure that the dataset contains a 'text' column.\n",
    "    if \"text\" not in dataset[\"train\"].column_names:\n",
    "        raise ValueError(\"The CSV file must contain a 'text' column.\")\n",
    "    \n",
    "    # Extract texts from the dataset.\n",
    "    texts = dataset[\"train\"][\"text\"]\n",
    "    \n",
    "    # Create a WordPiece tokenizer with a designated unknown token.\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    \n",
    "    # Use a simple whitespace pre-tokenizer.\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    # Initialize the trainer with the desired vocabulary size and special tokens.\n",
    "    trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "    \n",
    "    # Train the tokenizer on the extracted texts.\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "    \n",
    "    # Save the trained tokenizer to the specified output path.\n",
    "    tokenizer.save(output_path)\n",
    "    print(f\"Tokenizer successfully saved to {output_path}\")\n",
    "\n",
    "# Example usage:\n",
    "csv_path = train_path  # Replace with your CSV file path\n",
    "train_wordpiece_tokenizer_from_dataset(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [7582, 15, 701, 365, 279, 33], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Load the tokenizer from the saved file.\n",
    "# You can specify special tokens as needed.\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "\n",
    "# Now you can use the tokenizer as usual.\n",
    "sample_text = \"Hello, how are you?\"\n",
    "encoded = tokenizer(sample_text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file as a dataset using Hugging Face Datasets library.\n",
    "# This creates a default \"train\" split.\n",
    "dataset = load_dataset(\"csv\", data_files=train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'keyword', 'location', 'text', 'target'],\n",
       "        num_rows: 7613\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1    None     None  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4    None     None             Forest fire near La Ronge Sask. Canada   \n",
       "2   5    None     None  All residents asked to 'shelter in place' are ...   \n",
       "3   6    None     None  13,000 people receive #wildfires evacuation or...   \n",
       "4   7    None     None  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 936 ms, sys: 1.69 s, total: 2.62 s\n",
      "Wall time: 539 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "trainer = trainers.WordPieceTrainer(\n",
    "    vocab_size = 30_000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "files = [train_path]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_nlp_getting_started",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
