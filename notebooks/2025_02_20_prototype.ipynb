{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype end to end process\n",
    "1. Save the train/val/test sets (which were generated from the train set)\n",
    "2. Then have a simple process to train on the train set, optimize on the val set, and then test on the holdout test set. I'll then test the outputs on the actual submission test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trav_nlp.misc import polars_train_val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = OmegaConf.create({\n",
    "\n",
    "#     'mlflow': {\n",
    "#         'host': '127.0.0.1',\n",
    "#         'port': '8080',\n",
    "#         'uri': 'http://127.0.0.1:8080' # TODO: Make this interpolated\n",
    "#     },\n",
    "\n",
    "#     'raw_data': {\n",
    "#         'train_path': '../data/train.csv',\n",
    "#         'test_path': '../data/test.csv',\n",
    "#         'sample_submission_path': '../data/sample_submission.csv',\n",
    "#     },\n",
    "#     # Split the train dataset into a train/val/test split\n",
    "#     'training_data': {\n",
    "#         'train_path': '../data/splits/train.parquet',\n",
    "#         'val_path': '../data/splits/val.parquet',\n",
    "#         'test_path': '../data/splits/test.parquet'\n",
    "#     },\n",
    "\n",
    "#     'params': {\n",
    "#         'train_frac': 0.8,\n",
    "#         'val_frac': 0.1,\n",
    "#         'test_frac': 0.1,\n",
    "#         'train_val_test_seed': 42,   \n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/cpb68sbx4_n58pgsk31yp0v40000gn/T/ipykernel_45406/3843117015.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path='../conf'):\n"
     ]
    }
   ],
   "source": [
    "with initialize(config_path='../conf'):\n",
    "    cfg = compose(config_name='config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.experiment.submit_to_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train/val/test splits if they don't already exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, I guess I'll have a large wrapper function which runs a single experiment\n",
    "1. I suppose the larger wrapper will be run_experiment() or something similar. Then within that run_experiment wrapper I can have various different types of pipelines to train and evaluate, etc.\n",
    "2. I'll start with the most simple pipeline I can do. An sklearn pipeline\n",
    "3. The general idea of this will be to run an experiment, get the results of the model experiment, at the very least on the hold out test set, and then also submit the results to kaggle and get the results of that submission as well.\n",
    "    - So, it'll be train, val, and hold-out test set performance in a chart. Then also I'll submit the kaggle and get that performance.\n",
    "4. So first I'll code up the various parts of the loop. \n",
    "5. Then I'll integrate MLFlow so that I can include all those results into a single chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from trav_nlp.misc import submit_to_kaggle, setup_logging\n",
    "from trav_nlp.pipeline import train, eval_df_test, generate_and_submit_to_kaggle\n",
    "\n",
    "from trav_nlp.pipeline import load_or_create_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:38:59 INFO: Logging is configured.\n"
     ]
    }
   ],
   "source": [
    "setup_logging()\n",
    "logging.info(\"Logging is configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = load_or_create_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>keyword</th><th>location</th><th>text</th><th>target</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>9853</td><td>&quot;trauma&quot;</td><td>null</td><td>&quot;Today was trauma on top of tra…</td><td>0</td></tr><tr><td>798</td><td>&quot;battle&quot;</td><td>null</td><td>&quot;Dragon Ball Z: Battle Of Gods …</td><td>0</td></tr><tr><td>9822</td><td>&quot;trauma&quot;</td><td>null</td><td>&quot;Hiroshima: They told me to pai…</td><td>1</td></tr><tr><td>1817</td><td>&quot;buildings%20on%20fire&quot;</td><td>&quot;New Hampshire&quot;</td><td>&quot;17 people displaced after 3-al…</td><td>1</td></tr><tr><td>6148</td><td>&quot;hijack&quot;</td><td>&quot;Nigeria&quot;</td><td>&quot;Criminals Who Hijack Lorries A…</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────┬───────────────────────┬───────────────┬─────────────────────────────────┬────────┐\n",
       "│ id   ┆ keyword               ┆ location      ┆ text                            ┆ target │\n",
       "│ ---  ┆ ---                   ┆ ---           ┆ ---                             ┆ ---    │\n",
       "│ i64  ┆ str                   ┆ str           ┆ str                             ┆ i64    │\n",
       "╞══════╪═══════════════════════╪═══════════════╪═════════════════════════════════╪════════╡\n",
       "│ 9853 ┆ trauma                ┆ null          ┆ Today was trauma on top of tra… ┆ 0      │\n",
       "│ 798  ┆ battle                ┆ null          ┆ Dragon Ball Z: Battle Of Gods … ┆ 0      │\n",
       "│ 9822 ┆ trauma                ┆ null          ┆ Hiroshima: They told me to pai… ┆ 1      │\n",
       "│ 1817 ┆ buildings%20on%20fire ┆ New Hampshire ┆ 17 people displaced after 3-al… ┆ 1      │\n",
       "│ 6148 ┆ hijack                ┆ Nigeria       ┆ Criminals Who Hijack Lorries A… ┆ 1      │\n",
       "└──────┴───────────────────────┴───────────────┴─────────────────────────────────┴────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import socket\n",
    "# import subprocess\n",
    "# import time\n",
    "\n",
    "# def is_port_in_use(port, host='localhost'):\n",
    "#     \"\"\"\n",
    "#     Check if a given port on the host is currently in use.\n",
    "#     Returns True if the port is open (i.e. something is listening).\n",
    "#     \"\"\"\n",
    "#     with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "#         # connect_ex returns 0 if the connection is successful\n",
    "#         return sock.connect_ex((host, port)) == 0\n",
    "\n",
    "# def start_mlflow_server(port=5000):\n",
    "#     \"\"\"\n",
    "#     Starts the MLflow server on the given port using a subprocess.\n",
    "#     This function assumes that MLflow is installed and available in your PATH.\n",
    "#     \"\"\"\n",
    "#     # Check if MLflow server is already running\n",
    "#     if is_port_in_use(port):\n",
    "#         print(f\"MLflow server already running on port {port}. Using the existing server.\")\n",
    "#         return\n",
    "\n",
    "#     command = ['mlflow', 'server', '--port', str(port)]\n",
    "#     print(f\"Starting MLflow server on port {port}...\")\n",
    "    \n",
    "#     # Start the server as a background process.\n",
    "#     process = subprocess.Popen(command)\n",
    "    \n",
    "#     # Optionally wait a short time to allow the server to initialize.\n",
    "#     time.sleep(5)\n",
    "    \n",
    "#     if is_port_in_use(port):\n",
    "#         print(\"MLflow server started successfully.\")\n",
    "#     else:\n",
    "#         print(\"Failed to start the MLflow server.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(cfg, run_submit_to_kaggle = False):\n",
    "    \"\"\"Train/optimize a model, and then report the results of the model training run. \n",
    "    Also save/return the scores on the test.csv file for submission to kaggle if the model\n",
    "    appears to perform well.\n",
    "\n",
    "    So I'll have a train_model function\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df_train, df_val, df_test = load_or_create_data(cfg)\n",
    "\n",
    "    pipeline = train(df_train, df_val)\n",
    "\n",
    "    eval_df_test(pipeline, df_test)\n",
    "\n",
    "    if run_submit_to_kaggle:\n",
    "        df_full_train = pl.concat([df_train, df_val, df_test])\n",
    "        full_pipeline = train(df_full_train)\n",
    "        generate_and_submit_to_kaggle(full_pipeline, cfg.raw_data.test_path, cfg.raw_data.sample_submission_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2614, number of negative: 3476\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1843\n",
      "[LightGBM] [Info] Number of data points in the train set: 6090, number of used features: 699\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429228 -> initscore=-0.285001\n",
      "[LightGBM] [Info] Start training from score -0.285001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:38:59 INFO: Train ROC: 0.9263938401965869\n",
      "2025-02-23 23:38:59 INFO: Val ROC: 0.8571826280623607\n",
      "2025-02-23 23:38:59 INFO: Test ROC: 0.8419177701317206\n"
     ]
    }
   ],
   "source": [
    "run_experiment(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trav_nlp.misc import flatten_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {\n",
    "    'level1': {\n",
    "        'param1': 12,\n",
    "        'param2': 13,\n",
    "        'param3': {\n",
    "            'param4': 14,\n",
    "            'param5': 15\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level1.param1': 12,\n",
       " 'level1.param2': 13,\n",
       " 'level1.param3.param4': 14,\n",
       " 'level1.param3.param5': 15}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_dict(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrun_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mexperiment_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnested\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mparent_run_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlog_system_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfluent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActiveRun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Start a new MLflow run, setting it as the active run under which metrics and parameters\n",
      "will be logged. The return value can be used as a context manager within a ``with`` block;\n",
      "otherwise, you must call ``end_run()`` to terminate the current run.\n",
      "\n",
      "If you pass a ``run_id`` or the ``MLFLOW_RUN_ID`` environment variable is set,\n",
      "``start_run`` attempts to resume a run with the specified run ID and\n",
      "other parameters are ignored. ``run_id`` takes precedence over ``MLFLOW_RUN_ID``.\n",
      "\n",
      "If resuming an existing run, the run status is set to ``RunStatus.RUNNING``.\n",
      "\n",
      "MLflow sets a variety of default tags on the run, as defined in\n",
      ":ref:`MLflow system tags <system_tags>`.\n",
      "\n",
      "Args:\n",
      "    run_id: If specified, get the run with the specified UUID and log parameters\n",
      "        and metrics under that run. The run's end time is unset and its status\n",
      "        is set to running, but the run's other attributes (``source_version``,\n",
      "        ``source_type``, etc.) are not changed.\n",
      "    experiment_id: ID of the experiment under which to create the current run (applicable\n",
      "        only when ``run_id`` is not specified). If ``experiment_id`` argument\n",
      "        is unspecified, will look for valid experiment in the following order:\n",
      "        activated using ``set_experiment``, ``MLFLOW_EXPERIMENT_NAME``\n",
      "        environment variable, ``MLFLOW_EXPERIMENT_ID`` environment variable,\n",
      "        or the default experiment as defined by the tracking server.\n",
      "    run_name: Name of new run. Used only when ``run_id`` is unspecified. If a new run is\n",
      "        created and ``run_name`` is not specified, a random name will be generated for the run.\n",
      "    nested: Controls whether run is nested in parent run. ``True`` creates a nested run.\n",
      "    parent_run_id: If specified, the current run will be nested under the the run with\n",
      "        the specified UUID. The parent run must be in the ACTIVE state.\n",
      "    tags: An optional dictionary of string keys and values to set as tags on the run.\n",
      "        If a run is being resumed, these tags are set on the resumed run. If a new run is\n",
      "        being created, these tags are set on the new run.\n",
      "    description: An optional string that populates the description box of the run.\n",
      "        If a run is being resumed, the description is set on the resumed run.\n",
      "        If a new run is being created, the description is set on the new run.\n",
      "    log_system_metrics: bool, defaults to None. If True, system metrics will be logged\n",
      "        to MLflow, e.g., cpu/gpu utilization. If None, we will check environment variable\n",
      "        `MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING` to determine whether to log system metrics.\n",
      "        System metrics logging is an experimental feature in MLflow 2.8 and subject to change.\n",
      "\n",
      "Returns:\n",
      "    :py:class:`mlflow.ActiveRun` object that acts as a context manager wrapping the\n",
      "    run's state.\n",
      "\n",
      ".. code-block:: python\n",
      "    :test:\n",
      "    :caption: Example\n",
      "\n",
      "    import mlflow\n",
      "\n",
      "    # Create nested runs\n",
      "    experiment_id = mlflow.create_experiment(\"experiment1\")\n",
      "    with mlflow.start_run(\n",
      "        run_name=\"PARENT_RUN\",\n",
      "        experiment_id=experiment_id,\n",
      "        tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
      "        description=\"parent\",\n",
      "    ) as parent_run:\n",
      "        mlflow.log_param(\"parent\", \"yes\")\n",
      "        with mlflow.start_run(\n",
      "            run_name=\"CHILD_RUN\",\n",
      "            experiment_id=experiment_id,\n",
      "            description=\"child\",\n",
      "            nested=True,\n",
      "        ) as child_run:\n",
      "            mlflow.log_param(\"child\", \"yes\")\n",
      "    print(\"parent run:\")\n",
      "    print(f\"run_id: {parent_run.info.run_id}\")\n",
      "    print(\"description: {}\".format(parent_run.data.tags.get(\"mlflow.note.content\")))\n",
      "    print(\"version tag value: {}\".format(parent_run.data.tags.get(\"version\")))\n",
      "    print(\"priority tag value: {}\".format(parent_run.data.tags.get(\"priority\")))\n",
      "    print(\"--\")\n",
      "\n",
      "    # Search all child runs with a parent id\n",
      "    query = f\"tags.mlflow.parentRunId = '{parent_run.info.run_id}'\"\n",
      "    results = mlflow.search_runs(experiment_ids=[experiment_id], filter_string=query)\n",
      "    print(\"child runs:\")\n",
      "    print(results[[\"run_id\", \"params.child\", \"tags.mlflow.runName\"]])\n",
      "\n",
      "    # Create a nested run under the existing parent run\n",
      "    with mlflow.start_run(\n",
      "        run_name=\"NEW_CHILD_RUN\",\n",
      "        experiment_id=experiment_id,\n",
      "        description=\"new child\",\n",
      "        parent_run_id=parent_run.info.run_id,\n",
      "    ) as child_run:\n",
      "        mlflow.log_param(\"new-child\", \"yes\")\n",
      "\n",
      ".. code-block:: text\n",
      "    :caption: Output\n",
      "\n",
      "    parent run:\n",
      "    run_id: 8979459433a24a52ab3be87a229a9cdf\n",
      "    description: starting a parent for experiment 7\n",
      "    version tag value: v1\n",
      "    priority tag value: P1\n",
      "    --\n",
      "    child runs:\n",
      "                                 run_id params.child tags.mlflow.runName\n",
      "    0  7d175204675e40328e46d9a6a5a7ee6a          yes           CHILD_RUN\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/mlflow/tracking/fluent.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "mlflow.start_run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trav_nlp.misc import verify_git_commit\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "There are uncommitted changes in '/Users/traviswhitfield/Documents/github/kaggle_nlp_getting_started/conf'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/github/kaggle_nlp_getting_started/trav_nlp/misc.py:249\u001b[0m, in \u001b[0;36mverify_git_commit\u001b[0;34m(*target_folders)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 249\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiff_cmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVNULL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVNULL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError:\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/subprocess.py:415\u001b[0m, in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m popenargs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, cmd)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['git', 'diff', '--exit-code', 'HEAD', '--', 'conf']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m folders \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../trav_nlp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../conf\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m \u001b[43mverify_git_commit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfolders\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/kaggle_nlp_getting_started/trav_nlp/misc.py:256\u001b[0m, in \u001b[0;36mverify_git_commit\u001b[0;34m(*target_folders)\u001b[0m\n\u001b[1;32m    249\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mcheck_call(\n\u001b[1;32m    250\u001b[0m         diff_cmd,\n\u001b[1;32m    251\u001b[0m         cwd\u001b[38;5;241m=\u001b[39mrepo_root,\n\u001b[1;32m    252\u001b[0m         stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL,\n\u001b[1;32m    253\u001b[0m         stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL,\n\u001b[1;32m    254\u001b[0m     )\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are uncommitted changes in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabs_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Check for untracked files within the target folder.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m ls_files_cmd \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mls-files\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     rel_folder,\n\u001b[1;32m    266\u001b[0m ]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: There are uncommitted changes in '/Users/traviswhitfield/Documents/github/kaggle_nlp_getting_started/conf'."
     ]
    }
   ],
   "source": [
    "folders = [\n",
    "    '../trav_nlp',\n",
    "    '../conf']\n",
    "verify_git_commit(*folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../conf\", job_name=\"run_pipeline\", version_base=None):\n",
    "    cfg = compose(config_name=\"config\", overrides=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_leaves: 31\n",
      "n_estimators: 200\n",
      "learning_rate: 0.15\n",
      "max_depth: -1\n",
      "random_state: 42\n",
      "boosting_type: gbdt\n",
      "verbose: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.model_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'num_leaves': 12, 'n_estimators': 55}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model_params.update(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_leaves: 12\n",
      "n_estimators: 55\n",
      "learning_rate: 0.15\n",
      "max_depth: -1\n",
      "random_state: 42\n",
      "boosting_type: gbdt\n",
      "verbose: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(OmegaConf.to_yaml(cfg.model_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try using gensim and downloading GloVe word embeddings\n",
    "1. Then for the features of my model I'm going to take the average word embedding of the words in the tweet, and use that as the feature.\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import kaggle\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import optuna\n",
    "import polars as pl\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from prefect import flow, task\n",
    "from prefect.cache_policies import DEFAULT, INPUTS\n",
    "from prefect.context import get_run_context  # New import\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from trav_nlp.misc import (\n",
    "    flatten_dict,\n",
    "    polars_train_test_split,\n",
    "    polars_train_val_test_split,\n",
    "    submit_to_kaggle,\n",
    "    verify_git_commit,\n",
    ")\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trav_nlp.pipeline import train_val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(config_path=\"../conf\", job_name=\"run_pipeline\", version_base=None):\n",
    "    cfg = compose(config_name=\"config\", overrides=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENSIM_EMBEDDING_NAME = 'glove-twitter-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
      "CPU times: user 11.5 s, sys: 1.51 s, total: 13.1 s\n",
      "Wall time: 47.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove_vectors = downloader.load(GENSIM_EMBEDDING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = downloader.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext-wiki-news-subwords-300 1005.007116mb\n",
      "conceptnet-numberbatch-17-06-300 1225.497562mb\n",
      "word2vec-ruscorpora-300 208.427381mb\n",
      "word2vec-google-news-300 1743.56384mb\n",
      "glove-wiki-gigaword-50 69.182535mb\n",
      "glove-wiki-gigaword-100 134.300434mb\n",
      "glove-wiki-gigaword-200 264.336934mb\n",
      "glove-wiki-gigaword-300 394.362229mb\n",
      "glove-twitter-25 109.885004mb\n",
      "glove-twitter-50 209.216938mb\n",
      "glove-twitter-100 405.932991mb\n",
      "glove-twitter-200 795.3731mb\n",
      "__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "for key, val in info['models'].items():\n",
    "    try:\n",
    "        print(key, f\"{val['file_size']/1e6}mb\")\n",
    "    except:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Prefect logging level: INFO\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=fasttext-wiki-news-subwords-300\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=conceptnet-numberbatch-17-06-300\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=word2vec-ruscorpora-300\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=word2vec-google-news-300\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=glove-wiki-gigaword-50\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=glove-wiki-gigaword-100\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=glove-wiki-gigaword-200\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=glove-wiki-gigaword-300\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=glove-twitter-25\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=glove-twitter-50\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=glove-twitter-100\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=glove-twitter-200\n",
      "python trav_nlp/pipeline.py embeddings=glove1 embeddings.name=__testing_word2vec-matrix-synopsis\n"
     ]
    }
   ],
   "source": [
    "for key in info['models']:\n",
    "    print(f\"python trav_nlp/pipeline.py embeddings=glove1 embeddings.name={key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msynchronous\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtimestamp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrun_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_operations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunOperations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Log a metric under the current run. If no run is active, this method will create\n",
      "a new active run.\n",
      "\n",
      "Args:\n",
      "    key: Metric name. This string may only contain alphanumerics, underscores (_),\n",
      "        dashes (-), periods (.), spaces ( ), and slashes (/).\n",
      "        All backend stores will support keys up to length 250, but some may\n",
      "        support larger keys.\n",
      "    value: Metric value. Note that some special values such as +/- Infinity may be\n",
      "        replaced by other values depending on the store. For example, the\n",
      "        SQLAlchemy store replaces +/- Infinity with max / min float values.\n",
      "        All backend stores will support values up to length 5000, but some\n",
      "        may support larger values.\n",
      "    step: Metric step. Defaults to zero if unspecified.\n",
      "    synchronous: *Experimental* If True, blocks until the metric is logged\n",
      "        successfully. If False, logs the metric asynchronously and\n",
      "        returns a future representing the logging operation. If None, read from environment\n",
      "        variable `MLFLOW_ENABLE_ASYNC_LOGGING`, which defaults to False if not set.\n",
      "    timestamp: Time when this metric was calculated. Defaults to the current system time.\n",
      "    run_id: If specified, log the metric to the specified run. If not specified, log the metric\n",
      "        to the currently active run.\n",
      "\n",
      "Returns:\n",
      "    When `synchronous=True`, returns None.\n",
      "    When `synchronous=False`, returns `RunOperations` that represents future for\n",
      "    logging operation.\n",
      "\n",
      ".. code-block:: python\n",
      "    :test:\n",
      "    :caption: Example\n",
      "\n",
      "    import mlflow\n",
      "\n",
      "    # Log a metric\n",
      "    with mlflow.start_run():\n",
      "        mlflow.log_metric(\"mse\", 2500.00)\n",
      "\n",
      "    # Log a metric in async fashion.\n",
      "    with mlflow.start_run():\n",
      "        mlflow.log_metric(\"mse\", 2500.00, synchronous=False)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/mlflow/tracking/fluent.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "mlflow.log_metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fasttext-wiki-news-subwords-300': {'num_records': 999999,\n",
       "  'file_size': 1005007116,\n",
       "  'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py',\n",
       "  'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
       "  'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
       "   'https://arxiv.org/abs/1712.09405',\n",
       "   'https://arxiv.org/abs/1607.01759'],\n",
       "  'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
       "  'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
       "  'parts': 1},\n",
       " 'conceptnet-numberbatch-17-06-300': {'num_records': 1917247,\n",
       "  'file_size': 1225497562,\n",
       "  'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py',\n",
       "  'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
       "  'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
       "   'https://github.com/commonsense/conceptnet-numberbatch',\n",
       "   'http://conceptnet.io/'],\n",
       "  'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
       "  'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
       "  'parts': 1},\n",
       " 'word2vec-ruscorpora-300': {'num_records': 184973,\n",
       "  'file_size': 208427381,\n",
       "  'base_dataset': 'Russian National Corpus (about 250M words)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py',\n",
       "  'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
       "  'parameters': {'dimension': 300, 'window_size': 10},\n",
       "  'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
       "  'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
       "  'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
       "   'http://rusvectores.org/en/',\n",
       "   'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
       "  'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
       "  'file_name': 'word2vec-ruscorpora-300.gz',\n",
       "  'parts': 1},\n",
       " 'word2vec-google-news-300': {'num_records': 3000000,\n",
       "  'file_size': 1743563840,\n",
       "  'base_dataset': 'Google News (about 100 billion words)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py',\n",
       "  'license': 'not found',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
       "  'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
       "   'https://arxiv.org/abs/1301.3781',\n",
       "   'https://arxiv.org/abs/1310.4546',\n",
       "   'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
       "  'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
       "  'file_name': 'word2vec-google-news-300.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-50': {'num_records': 400000,\n",
       "  'file_size': 69182535,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 50},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
       "  'file_name': 'glove-wiki-gigaword-50.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-100': {'num_records': 400000,\n",
       "  'file_size': 134300434,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 100},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
       "  'file_name': 'glove-wiki-gigaword-100.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-200': {'num_records': 400000,\n",
       "  'file_size': 264336934,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 200},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
       "  'file_name': 'glove-wiki-gigaword-200.gz',\n",
       "  'parts': 1},\n",
       " 'glove-wiki-gigaword-300': {'num_records': 400000,\n",
       "  'file_size': 394362229,\n",
       "  'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 300},\n",
       "  'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
       "  'file_name': 'glove-wiki-gigaword-300.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-25': {'num_records': 1193514,\n",
       "  'file_size': 109885004,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 25},\n",
       "  'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
       "  'file_name': 'glove-twitter-25.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-50': {'num_records': 1193514,\n",
       "  'file_size': 209216938,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 50},\n",
       "  'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
       "  'file_name': 'glove-twitter-50.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-100': {'num_records': 1193514,\n",
       "  'file_size': 405932991,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 100},\n",
       "  'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
       "  'file_name': 'glove-twitter-100.gz',\n",
       "  'parts': 1},\n",
       " 'glove-twitter-200': {'num_records': 1193514,\n",
       "  'file_size': 795373100,\n",
       "  'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
       "  'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py',\n",
       "  'license': 'http://opendatacommons.org/licenses/pddl/',\n",
       "  'parameters': {'dimension': 200},\n",
       "  'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
       "  'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
       "  'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
       "   'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
       "  'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
       "  'file_name': 'glove-twitter-200.gz',\n",
       "  'parts': 1},\n",
       " '__testing_word2vec-matrix-synopsis': {'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
       "  'parameters': {'dimensions': 50},\n",
       "  'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
       "  'read_more': [],\n",
       "  'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
       "  'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
       "  'parts': 1}}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info['models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of trav_nlp.pipeline failed: Traceback (most recent call last):\n",
      "  File \"/Users/traviswhitfield/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/traviswhitfield/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/Users/traviswhitfield/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/importlib/__init__.py\", line 131, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 866, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  File \"/Users/traviswhitfield/Documents/github/kaggle_nlp_getting_started/trav_nlp/pipeline.py\", line 22, in <module>\n",
      "    from prefect.logging import configure_logging\n",
      "ImportError: cannot import name 'configure_logging' from 'prefect.logging' (/Users/traviswhitfield/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/prefect/logging/__init__.py)\n",
      "]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'configure_logging' from 'prefect.logging' (/Users/traviswhitfield/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/prefect/logging/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprefect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m configure_logging\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'configure_logging' from 'prefect.logging' (/Users/traviswhitfield/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/prefect/logging/__init__.py)"
     ]
    }
   ],
   "source": [
    "from prefect.logging import configure_logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prefect.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        module\n",
      "\u001b[0;31mString form:\u001b[0m <module 'prefect.logging' from '/Users/traviswhitfield/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/prefect/logging/__init__.py'>\n",
      "\u001b[0;31mFile:\u001b[0m        ~/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/prefect/logging/__init__.py\n",
      "\u001b[0;31mDocstring:\u001b[0m   <no docstring>"
     ]
    }
   ],
   "source": [
    "prefect.logging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom log format\n",
    "log_format = \"%(asctime)s | %(levelname)s | %(name)s | %(message)s\"\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "# Configure Prefect logging\n",
    "logging.basicConfig(format=log_format, datefmt=date_format, level=logging.INFO)\n",
    "\n",
    "# Reconfigure Prefect to use the new format\n",
    "configure_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 13\u001b[0m\n\u001b[1;32m      7\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m      8\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[1;32m      9\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, LogisticRegression())\n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Fit the pipeline\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get probabilities\u001b[39;00m\n\u001b[1;32m     16\u001b[0m y_probs \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict_proba(X_val)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Create an sklearn pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get probabilities\n",
    "y_probs = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Set a custom threshold (e.g., found via optimization)\n",
    "custom_threshold = 0.4  # Example threshold\n",
    "y_pred = (y_probs >= custom_threshold).astype(int)\n",
    "\n",
    "# Compute F1-score\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trav_nlp.pipeline import (\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_nlp_getting_started",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
