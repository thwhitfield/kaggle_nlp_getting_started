{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype end to end process\n",
    "1. Save the train/val/test sets (which were generated from the train set)\n",
    "2. Then have a simple process to train on the train set, optimize on the val set, and then test on the holdout test set. I'll then test the outputs on the actual submission test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trav_nlp.misc import polars_train_val_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = OmegaConf.create({\n",
    "\n",
    "#     'mlflow': {\n",
    "#         'host': '127.0.0.1',\n",
    "#         'port': '8080',\n",
    "#         'uri': 'http://127.0.0.1:8080' # TODO: Make this interpolated\n",
    "#     },\n",
    "\n",
    "#     'raw_data': {\n",
    "#         'train_path': '../data/train.csv',\n",
    "#         'test_path': '../data/test.csv',\n",
    "#         'sample_submission_path': '../data/sample_submission.csv',\n",
    "#     },\n",
    "#     # Split the train dataset into a train/val/test split\n",
    "#     'training_data': {\n",
    "#         'train_path': '../data/splits/train.parquet',\n",
    "#         'val_path': '../data/splits/val.parquet',\n",
    "#         'test_path': '../data/splits/test.parquet'\n",
    "#     },\n",
    "\n",
    "#     'params': {\n",
    "#         'train_frac': 0.8,\n",
    "#         'val_frac': 0.1,\n",
    "#         'test_frac': 0.1,\n",
    "#         'train_val_test_seed': 42,   \n",
    "#     }\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import compose, initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9z/cpb68sbx4_n58pgsk31yp0v40000gn/T/ipykernel_45406/3843117015.py:1: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path='../conf'):\n"
     ]
    }
   ],
   "source": [
    "with initialize(config_path='../conf'):\n",
    "    cfg = compose(config_name='config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.experiment.submit_to_kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the train/val/test splits if they don't already exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, I guess I'll have a large wrapper function which runs a single experiment\n",
    "1. I suppose the larger wrapper will be run_experiment() or something similar. Then within that run_experiment wrapper I can have various different types of pipelines to train and evaluate, etc.\n",
    "2. I'll start with the most simple pipeline I can do. An sklearn pipeline\n",
    "3. The general idea of this will be to run an experiment, get the results of the model experiment, at the very least on the hold out test set, and then also submit the results to kaggle and get the results of that submission as well.\n",
    "    - So, it'll be train, val, and hold-out test set performance in a chart. Then also I'll submit the kaggle and get that performance.\n",
    "4. So first I'll code up the various parts of the loop. \n",
    "5. Then I'll integrate MLFlow so that I can include all those results into a single chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from trav_nlp.misc import submit_to_kaggle, setup_logging\n",
    "from trav_nlp.pipeline import train, eval_df_test, generate_and_submit_to_kaggle\n",
    "\n",
    "from trav_nlp.pipeline import load_or_create_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:38:59 INFO: Logging is configured.\n"
     ]
    }
   ],
   "source": [
    "setup_logging()\n",
    "logging.info(\"Logging is configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = load_or_create_data(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>id</th><th>keyword</th><th>location</th><th>text</th><th>target</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>9853</td><td>&quot;trauma&quot;</td><td>null</td><td>&quot;Today was trauma on top of tra…</td><td>0</td></tr><tr><td>798</td><td>&quot;battle&quot;</td><td>null</td><td>&quot;Dragon Ball Z: Battle Of Gods …</td><td>0</td></tr><tr><td>9822</td><td>&quot;trauma&quot;</td><td>null</td><td>&quot;Hiroshima: They told me to pai…</td><td>1</td></tr><tr><td>1817</td><td>&quot;buildings%20on%20fire&quot;</td><td>&quot;New Hampshire&quot;</td><td>&quot;17 people displaced after 3-al…</td><td>1</td></tr><tr><td>6148</td><td>&quot;hijack&quot;</td><td>&quot;Nigeria&quot;</td><td>&quot;Criminals Who Hijack Lorries A…</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────┬───────────────────────┬───────────────┬─────────────────────────────────┬────────┐\n",
       "│ id   ┆ keyword               ┆ location      ┆ text                            ┆ target │\n",
       "│ ---  ┆ ---                   ┆ ---           ┆ ---                             ┆ ---    │\n",
       "│ i64  ┆ str                   ┆ str           ┆ str                             ┆ i64    │\n",
       "╞══════╪═══════════════════════╪═══════════════╪═════════════════════════════════╪════════╡\n",
       "│ 9853 ┆ trauma                ┆ null          ┆ Today was trauma on top of tra… ┆ 0      │\n",
       "│ 798  ┆ battle                ┆ null          ┆ Dragon Ball Z: Battle Of Gods … ┆ 0      │\n",
       "│ 9822 ┆ trauma                ┆ null          ┆ Hiroshima: They told me to pai… ┆ 1      │\n",
       "│ 1817 ┆ buildings%20on%20fire ┆ New Hampshire ┆ 17 people displaced after 3-al… ┆ 1      │\n",
       "│ 6148 ┆ hijack                ┆ Nigeria       ┆ Criminals Who Hijack Lorries A… ┆ 1      │\n",
       "└──────┴───────────────────────┴───────────────┴─────────────────────────────────┴────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import socket\n",
    "# import subprocess\n",
    "# import time\n",
    "\n",
    "# def is_port_in_use(port, host='localhost'):\n",
    "#     \"\"\"\n",
    "#     Check if a given port on the host is currently in use.\n",
    "#     Returns True if the port is open (i.e. something is listening).\n",
    "#     \"\"\"\n",
    "#     with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "#         # connect_ex returns 0 if the connection is successful\n",
    "#         return sock.connect_ex((host, port)) == 0\n",
    "\n",
    "# def start_mlflow_server(port=5000):\n",
    "#     \"\"\"\n",
    "#     Starts the MLflow server on the given port using a subprocess.\n",
    "#     This function assumes that MLflow is installed and available in your PATH.\n",
    "#     \"\"\"\n",
    "#     # Check if MLflow server is already running\n",
    "#     if is_port_in_use(port):\n",
    "#         print(f\"MLflow server already running on port {port}. Using the existing server.\")\n",
    "#         return\n",
    "\n",
    "#     command = ['mlflow', 'server', '--port', str(port)]\n",
    "#     print(f\"Starting MLflow server on port {port}...\")\n",
    "    \n",
    "#     # Start the server as a background process.\n",
    "#     process = subprocess.Popen(command)\n",
    "    \n",
    "#     # Optionally wait a short time to allow the server to initialize.\n",
    "#     time.sleep(5)\n",
    "    \n",
    "#     if is_port_in_use(port):\n",
    "#         print(\"MLflow server started successfully.\")\n",
    "#     else:\n",
    "#         print(\"Failed to start the MLflow server.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(cfg, run_submit_to_kaggle = False):\n",
    "    \"\"\"Train/optimize a model, and then report the results of the model training run. \n",
    "    Also save/return the scores on the test.csv file for submission to kaggle if the model\n",
    "    appears to perform well.\n",
    "\n",
    "    So I'll have a train_model function\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    df_train, df_val, df_test = load_or_create_data(cfg)\n",
    "\n",
    "    pipeline = train(df_train, df_val)\n",
    "\n",
    "    eval_df_test(pipeline, df_test)\n",
    "\n",
    "    if run_submit_to_kaggle:\n",
    "        df_full_train = pl.concat([df_train, df_val, df_test])\n",
    "        full_pipeline = train(df_full_train)\n",
    "        generate_and_submit_to_kaggle(full_pipeline, cfg.raw_data.test_path, cfg.raw_data.sample_submission_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2614, number of negative: 3476\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1843\n",
      "[LightGBM] [Info] Number of data points in the train set: 6090, number of used features: 699\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.429228 -> initscore=-0.285001\n",
      "[LightGBM] [Info] Start training from score -0.285001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-23 23:38:59 INFO: Train ROC: 0.9263938401965869\n",
      "2025-02-23 23:38:59 INFO: Val ROC: 0.8571826280623607\n",
      "2025-02-23 23:38:59 INFO: Test ROC: 0.8419177701317206\n"
     ]
    }
   ],
   "source": [
    "run_experiment(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trav_nlp.misc import flatten_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {\n",
    "    'level1': {\n",
    "        'param1': 12,\n",
    "        'param2': 13,\n",
    "        'param3': {\n",
    "            'param4': 14,\n",
    "            'param5': 15\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'level1.param1': 12,\n",
       " 'level1.param2': 13,\n",
       " 'level1.param3.param4': 14,\n",
       " 'level1.param3.param5': 15}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_dict(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrun_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mexperiment_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mrun_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnested\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mparent_run_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdescription\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlog_system_metrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfluent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActiveRun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Start a new MLflow run, setting it as the active run under which metrics and parameters\n",
      "will be logged. The return value can be used as a context manager within a ``with`` block;\n",
      "otherwise, you must call ``end_run()`` to terminate the current run.\n",
      "\n",
      "If you pass a ``run_id`` or the ``MLFLOW_RUN_ID`` environment variable is set,\n",
      "``start_run`` attempts to resume a run with the specified run ID and\n",
      "other parameters are ignored. ``run_id`` takes precedence over ``MLFLOW_RUN_ID``.\n",
      "\n",
      "If resuming an existing run, the run status is set to ``RunStatus.RUNNING``.\n",
      "\n",
      "MLflow sets a variety of default tags on the run, as defined in\n",
      ":ref:`MLflow system tags <system_tags>`.\n",
      "\n",
      "Args:\n",
      "    run_id: If specified, get the run with the specified UUID and log parameters\n",
      "        and metrics under that run. The run's end time is unset and its status\n",
      "        is set to running, but the run's other attributes (``source_version``,\n",
      "        ``source_type``, etc.) are not changed.\n",
      "    experiment_id: ID of the experiment under which to create the current run (applicable\n",
      "        only when ``run_id`` is not specified). If ``experiment_id`` argument\n",
      "        is unspecified, will look for valid experiment in the following order:\n",
      "        activated using ``set_experiment``, ``MLFLOW_EXPERIMENT_NAME``\n",
      "        environment variable, ``MLFLOW_EXPERIMENT_ID`` environment variable,\n",
      "        or the default experiment as defined by the tracking server.\n",
      "    run_name: Name of new run. Used only when ``run_id`` is unspecified. If a new run is\n",
      "        created and ``run_name`` is not specified, a random name will be generated for the run.\n",
      "    nested: Controls whether run is nested in parent run. ``True`` creates a nested run.\n",
      "    parent_run_id: If specified, the current run will be nested under the the run with\n",
      "        the specified UUID. The parent run must be in the ACTIVE state.\n",
      "    tags: An optional dictionary of string keys and values to set as tags on the run.\n",
      "        If a run is being resumed, these tags are set on the resumed run. If a new run is\n",
      "        being created, these tags are set on the new run.\n",
      "    description: An optional string that populates the description box of the run.\n",
      "        If a run is being resumed, the description is set on the resumed run.\n",
      "        If a new run is being created, the description is set on the new run.\n",
      "    log_system_metrics: bool, defaults to None. If True, system metrics will be logged\n",
      "        to MLflow, e.g., cpu/gpu utilization. If None, we will check environment variable\n",
      "        `MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING` to determine whether to log system metrics.\n",
      "        System metrics logging is an experimental feature in MLflow 2.8 and subject to change.\n",
      "\n",
      "Returns:\n",
      "    :py:class:`mlflow.ActiveRun` object that acts as a context manager wrapping the\n",
      "    run's state.\n",
      "\n",
      ".. code-block:: python\n",
      "    :test:\n",
      "    :caption: Example\n",
      "\n",
      "    import mlflow\n",
      "\n",
      "    # Create nested runs\n",
      "    experiment_id = mlflow.create_experiment(\"experiment1\")\n",
      "    with mlflow.start_run(\n",
      "        run_name=\"PARENT_RUN\",\n",
      "        experiment_id=experiment_id,\n",
      "        tags={\"version\": \"v1\", \"priority\": \"P1\"},\n",
      "        description=\"parent\",\n",
      "    ) as parent_run:\n",
      "        mlflow.log_param(\"parent\", \"yes\")\n",
      "        with mlflow.start_run(\n",
      "            run_name=\"CHILD_RUN\",\n",
      "            experiment_id=experiment_id,\n",
      "            description=\"child\",\n",
      "            nested=True,\n",
      "        ) as child_run:\n",
      "            mlflow.log_param(\"child\", \"yes\")\n",
      "    print(\"parent run:\")\n",
      "    print(f\"run_id: {parent_run.info.run_id}\")\n",
      "    print(\"description: {}\".format(parent_run.data.tags.get(\"mlflow.note.content\")))\n",
      "    print(\"version tag value: {}\".format(parent_run.data.tags.get(\"version\")))\n",
      "    print(\"priority tag value: {}\".format(parent_run.data.tags.get(\"priority\")))\n",
      "    print(\"--\")\n",
      "\n",
      "    # Search all child runs with a parent id\n",
      "    query = f\"tags.mlflow.parentRunId = '{parent_run.info.run_id}'\"\n",
      "    results = mlflow.search_runs(experiment_ids=[experiment_id], filter_string=query)\n",
      "    print(\"child runs:\")\n",
      "    print(results[[\"run_id\", \"params.child\", \"tags.mlflow.runName\"]])\n",
      "\n",
      "    # Create a nested run under the existing parent run\n",
      "    with mlflow.start_run(\n",
      "        run_name=\"NEW_CHILD_RUN\",\n",
      "        experiment_id=experiment_id,\n",
      "        description=\"new child\",\n",
      "        parent_run_id=parent_run.info.run_id,\n",
      "    ) as child_run:\n",
      "        mlflow.log_param(\"new-child\", \"yes\")\n",
      "\n",
      ".. code-block:: text\n",
      "    :caption: Output\n",
      "\n",
      "    parent run:\n",
      "    run_id: 8979459433a24a52ab3be87a229a9cdf\n",
      "    description: starting a parent for experiment 7\n",
      "    version tag value: v1\n",
      "    priority tag value: P1\n",
      "    --\n",
      "    child runs:\n",
      "                                 run_id params.child tags.mlflow.runName\n",
      "    0  7d175204675e40328e46d9a6a5a7ee6a          yes           CHILD_RUN\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/kaggle_nlp_getting_started/lib/python3.12/site-packages/mlflow/tracking/fluent.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "mlflow.start_run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_nlp_getting_started",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
